<!doctype html><html class=min-height-full lang=en><meta charset=utf-8><meta content="width=device-width,initial-scale=1,shrink-to-fit=no" name=viewport><meta content="light dark" name=color-scheme><link href=/favicon.ico rel=icon type=image/x-icon><link href=//fonts.gstatic.com/s/atkinsonhyperlegible/v10/9Bt23C1KxNDXMspQ1lPyU89-1h6ONRlW45G04pIo.woff2 rel=preload type=font/woff2 as=font crossorigin><link href=/feed.xml rel=alternate type=application/atom+xml title="Alex Saveau"><title>Your MPSC/SPMC/MPMC queue is not a queue | Alex Saveau</title><meta content="Jekyll v4.3.2" name=generator><meta content="Your MPSC/SPMC/MPMC queue is not a queue" property=og:title><meta content="Alex Saveau" name=author><meta content=en_US property=og:locale><meta content="Lockless queues let multiple cores communicate with each other without mutexes, typically to move work around for parallel processing. They come in four variants: {single,multi}-producer {single,multi}-consumer. A producer gives data to a consumer, each of which can be limited to a single thread (i.e. a single-{producer,consumer}) or shared across multiple threads. But only the single-producer single-consumer (SPSC) queue is actually a queue!" name=description><meta content="Lockless queues let multiple cores communicate with each other without mutexes, typically to move work around for parallel processing. They come in four variants: {single,multi}-producer {single,multi}-consumer. A producer gives data to a consumer, each of which can be limited to a single thread (i.e. a single-{producer,consumer}) or shared across multiple threads. But only the single-producer single-consumer (SPSC) queue is actually a queue!" property=og:description><link href=https://alexsaveau.dev/blog/opinions/performance/lockness/lockless-queues-are-not-queues rel=canonical><meta content=https://alexsaveau.dev/blog/opinions/performance/lockness/lockless-queues-are-not-queues property=og:url><meta content="Alex Saveau" property=og:site_name><meta content=https://alexsaveau.dev/assets/resized/me2-800-min.jpg property=og:image><meta content=article property=og:type><meta content=2025-08-16T00:00:00+00:00 property=article:published_time><meta content=summary_large_image name=twitter:card><meta content=https://alexsaveau.dev/assets/resized/me2-800-min.jpg property=twitter:image><meta content="Your MPSC/SPMC/MPMC queue is not a queue" property=twitter:title><meta content=@SUPERCILEX name=twitter:site><meta content=@SUPERCILEX name=twitter:creator><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Alex Saveau"},"dateModified":"2025-09-25T00:58:09+00:00","datePublished":"2025-08-16T00:00:00+00:00","description":"Lockless queues let multiple cores communicate with each other without mutexes, typically to move work around for parallel processing. They come in four variants: {single,multi}-producer {single,multi}-consumer. A producer gives data to a consumer, each of which can be limited to a single thread (i.e. a single-{producer,consumer}) or shared across multiple threads. But only the single-producer single-consumer (SPSC) queue is actually a queue!","headline":"Your MPSC/SPMC/MPMC queue is not a queue","image":"https://alexsaveau.dev/assets/resized/me2-800-min.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://alexsaveau.dev/blog/opinions/performance/lockness/lockless-queues-are-not-queues"},"url":"https://alexsaveau.dev/blog/opinions/performance/lockness/lockless-queues-are-not-queues"}</script><script>window.addEventListener("DOMContentLoaded",()=>{for(let e of document.querySelectorAll(".loads")){var o=()=>{e.classList.remove("loads")};e.complete?o():(e.onload=o,e.onerror=o)}})</script><noscript><style>.loads{animation:none!important;background-color:unset!important}</style></noscript><link href=//fonts.gstatic.com/s/firacode/v17/uU9NCBsR6Z2vfE9aq3bh3dSD.woff2 rel=preload type=font/woff2 as=font crossorigin><style>@keyframes tooltip-appear{0%{opacity:0}to{opacity:1}}@keyframes pulse{0%,to{opacity:1}50%{opacity:.33}}:root{--progress-color:gray;--blockquote-color:#6a737d;--blockquote-boarder-color:#959da5;--code-background-color:rgba(0, 0, 0, 0.05);--hr-color:gray;--footer-color:white;--loading-pulse-color:#b0b0b0}@font-face{font-family:"Fira Code";font-style:normal;font-weight:475;font-display:swap;src:url(//fonts.gstatic.com/s/firacode/v17/uU9NCBsR6Z2vfE9aq3bh3dSD.woff2) format("woff2");unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}.article{overflow-wrap:break-word}.article .caption{font-size:1rem;margin:-1.5em 0 0}.article h2,.article h3{margin:1.5em 0-.5em}.article h2{font-size:1.875rem}.article h3{font-size:1.5rem}.article p{font-size:1.313rem;margin:.95em 0 1.2em}.article p.caption{text-align:center}.article img.article-image{width:100%;height:auto}.article strong{font-size:1.25rem}.article code{font-family:"Fira Code",monospace;font-size:1rem;background-color:var(--code-background-color);border-radius:3px;padding:2px 4px}.article blockquote{border-left:4px solid var(--blockquote-boarder-color);padding-left:16px;margin-bottom:16px;margin-left:0;margin-right:0}.article blockquote :not(a){color:var(--blockquote-color)}.article li ul,.article ol,.article ul{font-size:1.313rem;padding-left:32px;margin-bottom:16px}.article li ul{padding-left:16px;margin-bottom:0}.header-link{visibility:hidden}h2:hover .header-link,h3:hover .header-link{visibility:visible}@media (prefers-color-scheme:dark){:root{--progress-color:whitesmoke;--blockquote-color:#dadada;--blockquote-boarder-color:#dadada;--code-background-color:rgba(255,255,255,0.15)}.article .caption{color:#dadada}.article p{color:#ededed}}.octicon{display:inline-block;fill:currentColor;vertical-align:text-bottom}html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji";line-height:1.5;color:#24292e;background-color:#fff;font-family:Atkinson Hyperlegible,sans-serif;font-size:1em}footer{display:block;display:flex;padding:5px;align-items:center;background-color:var(--footer-color)}a{background-color:transparent;color:#0366d6;text-decoration:none}a:active,a:hover{outline-width:0}strong{font-weight:600}h1{margin:.67em 0}small{font-size:90%}img{border-style:none}svg:not(:root){overflow:hidden}code{font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,monospace;font-size:.75rem}::-webkit-input-placeholder{color:inherit;opacity:.54}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}*{box-sizing:border-box}a:hover{text-decoration:underline}h1,h2,h3,ol,p,ul{margin-top:0;margin-bottom:0}h1,h2,h3{font-size:2rem;font-weight:600}h2,h3{font-size:1.5rem}h3{font-size:1.25rem}ol,p,ul{margin-bottom:10px}blockquote{margin:0}ol,ul{padding-left:0;margin-bottom:0}:-ms-input-placeholder{color:#6a737d;opacity:1}::-ms-input-placeholder{color:#6a737d;opacity:1}:checked+.radio-label{position:relative;z-index:1;border-color:#0366d6}@media (min-width:768px){.col-md-5{width:41.66667%}.col-md-7{width:58.33333%}}@media (min-width:1012px){.col-lg-4{width:33.33333%}.col-lg-8{width:66.66667%}}@media (min-width:1280px){.col-xl-3{width:25%}.col-xl-9{width:75%}}.tooltipped{position:relative}.tooltipped::after,.tooltipped::before{position:absolute;display:none;pointer-events:none;opacity:0}.tooltipped::after{z-index:1000000;padding:.5em .75em;font:11px/1.5 -apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji";-webkit-font-smoothing:subpixel-antialiased;color:#fff;text-align:center;text-decoration:none;text-shadow:none;text-transform:none;letter-spacing:normal;word-wrap:break-word;white-space:pre;content:attr(aria-label);background:#1b1f23;border-radius:6px}.tooltipped::before{z-index:1000001;width:0;height:0;color:#1b1f23;content:"";border:6px solid transparent}.tooltipped:active::after,.tooltipped:active::before,.tooltipped:focus::after,.tooltipped:focus::before,.tooltipped:hover::after,.tooltipped:hover::before{display:inline-block;text-decoration:none;animation-name:tooltip-appear;animation-duration:.1s;animation-fill-mode:forwards;animation-timing-function:ease-in;animation-delay:.4s}.tooltipped-se::after{top:100%;margin-top:6px}.tooltipped-se::before{top:auto;right:50%;bottom:-7px;margin-right:-6px;border-bottom-color:#1b1f23}.tooltipped-se::after{right:auto;left:50%;margin-left:-16px}.border-top{border-top:1px solid #e1e4e8!important}@media (min-width:768px){.border-md-right{border-right:1px solid #e1e4e8!important}.border-md-bottom{border-bottom:1px solid #e1e4e8!important}.border-md-top-0{border-top:0!important}}.circle{border-radius:50%!important}.border-gray-light{border-color:#eaecef!important}.bg-white{background-color:#fff!important}.bg-gray-light{background-color:#fafbfc!important}.text-gray{color:#586069!important}.flex-wrap{flex-wrap:wrap!important}.flex-items-start{align-items:flex-start!important}.flex-items-center{align-items:center!important}.flex-self-stretch{align-self:stretch!important}.v-align-middle{vertical-align:middle!important}.mr-2{margin-right:8px!important}.mx-auto{margin-right:auto!important;margin-left:auto!important}.px-4{padding-right:24px!important;padding-left:24px!important}.f4{font-size:1rem!important}@media (min-width:768px){.px-md-6{padding-right:40px!important;padding-left:40px!important}.f4{font-size:1rem!important}}.f5{font-size:.875rem!important}.f00-light{font-size:2.5rem!important;font-weight:300!important}@media (min-width:768px){.f00-light{font-size:3rem!important}}.f2-light{font-size:1.375rem!important;font-weight:300!important}.lh-condensed{line-height:1.25!important}.d-flex{display:flex!important}.d-none{display:none!important}@font-face{font-family:Inter;font-style:normal;font-weight:400;src:local("Inter"),local("Inter-Regular"),url(/fonts/Inter-Regular.woff) format("woff");font-display:swap}@font-face{font-family:Inter;font-style:normal;font-weight:500;src:local("Inter Medium"),local("Inter-Medium"),url(/fonts/Inter-Medium.woff) format("woff");font-display:swap}@font-face{font-family:Inter;font-style:normal;font-weight:600;src:local("Inter Bold"),local("Inter-Bold"),url(/fonts/Inter-Bold.woff) format("woff");font-display:swap}.mb-2{margin-bottom:8px!important}.mb-3{margin-bottom:16px!important}.mb-5{margin-bottom:32px!important}.mb-6{margin-bottom:40px!important}.py-6{padding-top:40px!important;padding-bottom:40px!important}@font-face{font-family:"Atkinson Hyperlegible";font-style:normal;font-weight:400;font-display:swap;src:url(//fonts.gstatic.com/s/atkinsonhyperlegible/v10/9Bt23C1KxNDXMspQ1lPyU89-1h6ONRlW45G04pIo.woff2) format("woff2");unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}footer small{width:100%}footer small:last-child{text-align:end}.min-height-full{min-height:100vh}.icon-color{fill:#24292e!important}.loads{animation-duration:2s;animation:2s cubic-bezier(.4,0,.6,1) infinite pulse;animation-timing-function:cubic-bezier(.4,0,.6,1);animation-iteration-count:infinite;animation-name:pulse;background-color:var(--loading-pulse-color)}@media (min-width:768px){.f2-light{font-size:1.5rem!important}.d-md-flex{display:flex!important}#masthead{position:fixed;width:min-content}.masthead-name-mobile{display:none}}@media (max-width:768px){.masthead-mini{padding-top:10px!important;padding-bottom:10px!important}.masthead-mini-container{display:flex;align-items:center}.masthead-profile{width:80px;height:80px;margin-bottom:0!important}.masthead-intro{padding-left:24px}.masthead-name{display:none}.masthead-bio{margin-bottom:0!important}.masthead-metadata{display:none}}@media (min-width:1942px){.masthead{max-width:400px}.content-container{width:100%}}@media (prefers-color-scheme:dark){:root{--hr-color:whitesmoke;--footer-color:black;--loading-pulse-color:#383838}a{color:unset!important;text-decoration:underline}.bg-white{background-color:#4f565d!important}.bg-gray-light{background-color:#2f363d!important}.border-gray-light{border-color:transparent!important}.text-gray{color:#d0d8e1!important}.border-md-bottom{border:0!important}.masthead{background-color:#24292e!important}.scoped-text-defaults,.text-defaults,.text-defaults *{color:#fff!important}.icon-color{fill:#fff!important}}</style><body class=min-height-full id=top><div class="min-height-full bg-gray-light border-md-bottom d-md-flex"><div class="px-4 px-lg-7 py-6 bg-white border-gray-light border-md-right col-lg-4 col-md-5 col-xl-3 flex-self-stretch masthead masthead-mini px-md-6"><div class=masthead-mini-container id=masthead><a href=/ ><picture><source sizes=150px srcset="/assets/resized/me2-240-min.avif 240w, /assets/resized/me2-320-min.avif 320w, /assets/resized/me2-480-min.avif 480w, /assets/resized/me2-800-min.avif 800w, /assets/resized/me2-min.avif 1528w" type=image/avif><source sizes=150px srcset="/assets/resized/me2-240-min.webp 240w, /assets/resized/me2-320-min.webp 320w, /assets/resized/me2-480-min.webp 480w, /assets/resized/me2-800-min.webp 800w, /assets/resized/me2-min.webp 1528w" type=image/webp><source sizes=150px srcset="/assets/resized/me2-240-min.jpg 240w, /assets/resized/me2-320-min.jpg 320w, /assets/resized/me2-480-min.jpg 480w, /assets/resized/me2-800-min.jpg 800w, /assets/resized/me2-min.jpg 1528w" type=image/jpeg><source sizes=150px srcset="/assets/resized/me2-240.jpg 240w, /assets/resized/me2-320.jpg 320w, /assets/resized/me2-480.jpg 480w, /assets/resized/me2-800.jpg 800w, /assets/me2.jpg 1528w"><img alt="Profile Picture" class="mb-3 circle loads masthead-profile" height=150 loading=lazy src=/assets/resized/me2-240.jpg width=150></picture></a><div class=masthead-intro><h1 class="text-defaults lh-condensed mb-2 masthead-name">Alex Saveau</h1><h2 class="text-defaults lh-condensed mb-2 masthead-name-mobile">Alex Saveau</h2><p class="text-gray f4 masthead-bio mb-3" id=bio>Relentless efficiency</div><div class="f4 masthead-metadata"><div class="text-defaults d-flex flex-items-center mb-3"><svg height=20 viewBox="0 0 16 16" aria-label=GitHub class="icon-color mr-2 octicon v-align-middle octicon-mark-github" role=img version=1.1 width=20><path d="M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"></path></svg> <a href=//github.com/SUPERCILEX>@SUPERCILEX</a></div><div class="text-defaults d-flex flex-items-center mb-3"><svg height=20 viewBox="0 0 16 16" aria-label=email class="icon-color mr-2 octicon v-align-middle octicon-mail" role=img version=1.1 width=20><path d="M1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25v-8.5C0 2.784.784 2 1.75 2ZM1.5 12.251c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V5.809L8.38 9.397a.75.75 0 0 1-.76 0L1.5 5.809v6.442Zm13-8.181v-.32a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25v.32L8 7.88Z"></path></svg> <a href=mailto:saveau.alexandre@gmail.com>saveau.alexandre@gmail.com</a></div><div class="flex-items-start d-flex flex-wrap"><div class=mb-3><a href=//stackoverflow.com/u/4548500 class="tooltipped tooltipped-se" aria-label="Stack Overflow: 4548500"><svg height=24 viewBox="0 0 120 120" fill=#959da5 xmlns=http://www.w3.org/2000/svg><path d="M84.4 93.8V70.6h7.7v30.9H22.6V70.6h7.7v23.2z" class=st0 /><path d="M38.8 68.4l37.8 7.9 1.6-7.6-37.8-7.9-1.6 7.6zm5-18l35 16.3 3.2-7-35-16.4-3.2 7.1zm9.7-17.2l29.7 24.7 4.9-5.9-29.7-24.7-4.9 5.9zm19.2-18.3l-6.2 4.6 23 31 6.2-4.6-23-31zM38 86h38.6v-7.7H38V86z" class=st1 /></svg><span class=d-none>Stack Overflow</span></a></div></div></div></div></div><div class="px-4 px-lg-7 py-6 border-md-top-0 border-top col-lg-8 col-md-7 col-xl-9 content-container" id=article><div class=mx-auto style=max-width:900px><div class="f4 mb-6"><div class="f4 scoped-text-defaults"><p class=f5><a href=/blog class="text-defaults d-flex flex-items-center"><svg height=16 viewBox="0 0 16 16" aria-label=include.parent class="icon-color mr-2 octicon v-align-middle octicon-chevron-left" role=img version=1.1 width=16><path d="M9.78 12.78a.75.75 0 0 1-1.06 0L4.47 8.53a.75.75 0 0 1 0-1.06l4.25-4.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L6.06 8l3.72 3.72a.75.75 0 0 1 0 1.06Z"></path></svg>Blog</a><h1 class="text-defaults lh-condensed f00-light" style=font-weight:600!important>Your MPSC/SPMC/MPMC queue is not a queue</h1><h2 class="text-gray f2-light lh-condensed" style=font-weight:400!important>Rethinking our approach to lockless channels</h2><p class="text-gray mb-5">Published Aug 16, 2025 • Last updated Sep 25, 2025 • 10 min read<div class=article><p>Lockless queues let multiple cores communicate with each other without mutexes, typically to move work around for parallel processing. They come in four variants: <code class="highlighter-rouge language-plaintext">{single,multi}</code>-producer <code class="highlighter-rouge language-plaintext">{single,multi}</code>-consumer. A producer gives data to a consumer, each of which can be limited to a single thread (i.e. a single-<code class="highlighter-rouge language-plaintext">{producer,consumer}</code>) or shared across <strong>multi</strong>ple threads. But only the single-producer single-consumer (SPSC) queue is actually a queue!<blockquote><p>This article is part of <a href=/blog/tags/lockness>a series</a> building <a href=//github.com/SUPERCILEX/lockness/blob/master/README.md>Lockness</a>, a high-performance blocking task executor.</blockquote><style>.too-big img{max-height:50vh}</style><h2 id=spmcmpmc-queues-are-broken>SPMC/MPMC queues are broken <a href=#spmcmpmc-queues-are-broken class=header-link>#</a></h2><p>Consider the so-called SPMC queue. By definition, received messages cannot be processed in a total global order without additional external synchronization. First, the single, ordered input stream is arbitrarily split amongst each consumer. Then, each message is removed from the queue in the same order. But from this moment onwards, the consumer thread can be paused at any moment (even within the library implementation that is still copying the data into your code). Consequently, another thread can process an element from the future before your code has even had a chance to see that it claimed an element, now from the past.<p>Acausality <em>within</em> consumers is the only upheld invariant: a consumer will not see any elements prior to the last element it has seen. This guarantee is almost certainly too weak to be useful as consumers have no control over which set of elements are seen, meaning arbitrary elements from the future may have been processed on other threads.<div class=too-big><p><a href=/assets/resized/lockness/acausal-recv-min.svg><img alt="Diagram of a SPMC channel exhibiting non-queue-like behavior." class=article-image height=955 loading=lazy src=/assets/resized/lockness/acausal-recv-min.svg width=721></a><div class=text-gray><p class=caption>An example SPMC queue where element B is processed on thread 2 acausally before A on thread 1</div></div><p>Similar logic applies to MPMC channels with the additional weakening that different producer streams are processed in no particular order. To work around this, some implementations use many SPMC channels to make a MPMC channel. They introduce the concept of a token which lets consumers optionally choose a specific producer to consume. Were this token to guarantee exclusive access to the producer, you’ve just created a poor man’s SPSC queue. Without exclusivity, you get all the same problems as SPMC channels (items being processed out-of-order by other threads).<h2 id=mpsc-queues-are-special>MPSC queues are special <a href=#mpsc-queues-are-special class=header-link>#</a></h2><p>While additional synchronization can be applied on top of SPMC and MPMC channels to provide ordering guarantees, the more useful abstraction is a stream. MPSC channels are special in that each producer can be thought of as its own stream, even though no ordering guarantees are provided between streams (producers). The consumer will see each stream in order with interleavings between streams. In hardware terms, it’s a multiplexer.<p>Consumer threads can then be set up for specific purposes. For example, <a href=//github.com/SUPERCILEX/clipboard-history/blob/master/README.md>Ringboard</a> uses two consumer threads following the actor model in its UI implementations. Any thread can request state changes and/or submit view updates, but state changes and view updates are each processed serially on their own threads. Since I only have two consumer threads, this is effectively a mini model-view-controller framework: the controller thread handles model updates and the main thread updates the view. Notice that order within streams is important: the controller should process user input in the order in which actions occurred. However, other updates (i.e. other streams/producers) like an image loader having finished retrieving an image from a background thread can be interleaved arbitrarily with the user input stream.<p>Thus, MPSC channels as a whole aren’t queues, but each producer is its own queue which provides useful guarantees.<h2 id=the-rundown>The rundown <a href=#the-rundown class=header-link>#</a></h2><p>To summarize, SPMC queues and by extension MPMC queues don’t have useful ordering guarantees—calling them queues is silly. MPSC queues can be thought of as a set of producer queues multiplexed together.<blockquote><p>Note: I’ve left SPSC queues out of this discussion because they are real queues with a generally agreed upon optimal implementation: power-of-2 queue capacity backed by duplicated mmaps with cached head/tail pointers expressed in terms of elements written/read, and optional <a href=//man7.org/linux/man-pages/man2/get_robust_list.2.html>get_robust_list</a> support to handle multiprocess shared memory dead counterparty notification.</blockquote><h2 id=lockless-queues-are-slow>Lockless queues are slow <a href=#lockless-queues-are-slow class=header-link>#</a></h2><p>Lockless queues are so named by virtue of being implemented with a queue, namely circular buffers or variations on linked lists. This is problematic because the queue linearizes updates to the channel where no such global ordering can be observed as explained above.<div class=too-big id=mpmc-diagram><p><a href=/assets/resized/lockness/mpmc-blocking-min.svg><img alt="Diagram of MPMC channel contention." class=article-image height=2157 loading=lazy src=/assets/resized/lockness/mpmc-blocking-min.svg width=2352></a><div class=text-gray><p class=caption>Blocked producers and consumers in a MPMC channel, due to linearization</div></div><p>The implementation of a lockless queue can be conceptualized through four pointers:<ul><li>The tail: producers increment it to reserve a slot to work within.<ul><li>Slots in <span style=text-decoration:underline;text-decoration-color:#e03131>red</span> are being written to without interfering with consumers.</ul><li>The committed pointer: producers have finished writing to all slots past this offset.<ul><li>Slots in <span style=text-decoration:underline;text-decoration-color:#f08c00>orange</span> should be ready to be consumed, but the shaded slot hasn’t finished its write, thereby blocking consumers from accessing subsequent elements.</ul><li>The head: consumers increment it to claim a slot for consumption.<ul><li>Slots in <span style=text-decoration:underline;text-decoration-color:#2f9e44>green</span> are ready to be read without interfering with producers.</ul><li>The consumed pointer: consumers have finished reading all slots past this offset.<ul><li>Slots in <span style=text-decoration:underline;text-decoration-color:#1971c2>blue</span> should be ready to be written to, but the shaded slot hasn’t finished its read, thereby blocking producers from writing to subsequent elements.</ul></ul><p>This approach is not wait-free: a context-switched producer or consumer in the middle of writing or reading a value will prevent further progress.<h2 id=lockless-algorithm-fundamentals>Lockless algorithm fundamentals <a href=#lockless-algorithm-fundamentals class=header-link>#</a></h2><p>The core problem in lockless algorithms is mediating access to shared memory. SPSC queues have it easy: they can prepare work and only commit it once they’re ready. Once you allow multiple threads to compete for the ability to access the same memory, they must go through stages:<ol><li>A thread must exclude other competitors accessing a chunk of memory.<li>The thread uses the memory (non-atomically). This stage should be as fast as possible and is typically just a <code class="highlighter-rouge language-plaintext">memcpy</code>.<li>The thread commits (publishes) its change.</ol><p>Producers reserve memory to publish to consumers while consumers claim memory to be read and then released back to publishers.<h2 id=lockless-bags-as-a-new-approach>Lockless bags as a new approach <a href=#lockless-bags-as-a-new-approach class=header-link>#</a></h2><p>We’ve established that queue-based lockless channels pay the cost of linearizability without being able to take advantage of it. We’ve also seen that the only true requirement for a lockless channel is the ability to lock a region of memory.<p>Instead of a queue, let’s use a bag! What’s a bag? Well, uhhhh… It’s a bag. You can put stuff in, rummage around, and take stuff out. Notice that I said nothing about <em>what</em> you get out—if it’s in the bag, it’s a valid item to be taken out at any time (i.e. in an unspecified order).<p>The fastest single-threaded bag implementation is of course a stack. But this is the multithreaded world, so let’s instead use an array and two bitvectors. The first bitvector will be our reservations: producers atomically set bits to gain exclusive access to the corresponding array slot. The second bitvector is the list of committed slots: once producers are done with a slot, they set its corresponding commit bit. Conversely, consumers unset commit bits to read the slot and unset the reservation bit to return the slot to producers.<div class=too-big><p><a href=/assets/resized/lockness/mpmc-bag-min.svg><img alt="Diagram of a bag-based MPMC channel." class=article-image height=1210 loading=lazy src=/assets/resized/lockness/mpmc-bag-min.svg width=995></a><div class=text-gray><p class=caption>Threads in a lockless bag independently control their slots</div></div><p>In this scheme, every producer and consumer operates independently. If a thread is stuck between stages, it has no effect on the progress of other threads. We have ourselves a wait-free MPMC channel!<h2 id=you-dont-need-unbounded-channels>You don’t need unbounded channels <a href=#you-dont-need-unbounded-channels class=header-link>#</a></h2><p>Limitless anything doesn’t exist in the real world, as much as we love to pretend it does. Unbounded channels introduce a lot of complexity in an attempt to paper over poorly engineered systems. If consumers cannot keep up, producers must slow down. The best way to go about this is to apply backpressure, but sadly this is rarely an option. Dropping the messages is a possibility, though a distasteful one. Alternatively, producers can cheat and buffer messages locally while waiting for space to free up when consumers are full. This last approach is the one taken throughout Lockness, minimizing communication and therefore contention when it is most critical (the channel is overloaded).<p>For this reason (and let’s be real mostly because unbounded channels are hard), the lockless bags I’ve implemented are unconfigurably bounded.<h3 id=but-heres-an-idea-to-support-unbounded-lockless-bags>But here’s an idea to support unbounded lockless bags <a href=#but-heres-an-idea-to-support-unbounded-lockless-bags class=header-link>#</a></h3><p>Make it a tree! Right now, the bitset represents data slots in an array, but it could additionally allow pointers to sub-nodes. This feels like a reasonable approach, though I haven’t thought too hard about a precise implementation.<h2 id=introducing-lockness-bags>Introducing Lockness Bags <a href=#introducing-lockness-bags class=header-link>#</a></h2><p><a href=//github.com/SUPERCILEX/lockness/blob/master/bags/README.md>Lock<strong>n</strong>ess Bags</a> implement the ideas described above and might be used to build the <a href=//github.com/SUPERCILEX/lockness/blob/master/executor/README.md>Lockness Executor</a> should benchmarking show an advantage.<h3 id=are-lockness-bags-fast>Are Lockness bags fast? <a href=#are-lockness-bags-fast class=header-link>#</a></h3><p>No :(<p>Queue-based channel implementations win over bags on current hardware by disjointing their producer and consumer memory writes. Remember the <code class="highlighter-rouge language-plaintext">consumed</code> and <code class="highlighter-rouge language-plaintext">committed</code> pointers from <a href=#mpmc-diagram>the diagram</a>? In practice, these are implemented in a distributed fashion: each slot has a flag that marks it as readable or writable. To start, all slots are writable and transition to readable and back as you write and read the slots. The head pointer can only advance if its current slot is readable and conversely for the tail pointer. Crucially, this means consuming a slot almost never touches a cache line producers are actively working with.<p>On the other hand, lockless bags are implemented with two atomics each representing a bitvector. To produce and consume a value, you must always update both bitvectors. This means producers <em>and</em> consumers are all contending over the same two cache lines. On the other hand, lockless queues limit contention for producers to the tail cache line and similarly consumers only contend on the head cache line.<h2 id=the-future-hardware-accelerating-lockless-bags>The future: hardware accelerating lockless bags <a href=#the-future-hardware-accelerating-lockless-bags class=header-link>#</a></h2><p>The <a href=atomic-bit-fill>next article</a> in the series explores the idea of novel instructions that would hardware accelerate lockless bags to significantly outperform all possible software channel implementations.<h2 id=appendix-alternative-approaches-and-their-pitfalls>Appendix: alternative approaches and their pitfalls <a href=#appendix-alternative-approaches-and-their-pitfalls class=header-link>#</a></h2><p>This problem has been itching the back of my brain for close to five years now. As part of the journey, I’ve toyed with many different approaches that were rejected.<h3 id=why-doesnt-a-stack-work-for-mpsc-channels>Why doesn’t a stack work for MPSC channels? <a href=#why-doesnt-a-stack-work-for-mpsc-channels class=header-link>#</a></h3><p>Instead of a circular buffer, couldn’t we use a stack? Producers would compete to place items at the top and the single consumer takes items down. Unfortunately, the consumer would need to block producers from raising the stack, otherwise the consumer could end up in a situation where it is trying to out an already stacked upon element. You can hack around this, but it doesn’t seem better than a queue.<h3 id=why-not-use-many-spsc-queues-to-make-a-mpsc-channel>Why not use many SPSC queues to make a MPSC channel? <a href=#why-not-use-many-spsc-queues-to-make-a-mpsc-channel class=header-link>#</a></h3><p>Generalizing the question, why not use multiple stricter channels to build a weaker one? On the surface, this appears to be a straightforward solution, but you run into two problems:<ul><li>Load balancing: it is difficult to share resources. For example, consider using many SPMC channels to make a MPMC channel. If one producer has a spike in load while the others remain quiet, there is no way to use the available capacity of the other producers’ channels.<li>Poor scaling with high core counts: either producing or consuming values must scale linearly with the number of threads to scan across the individual queues. That said, this can be worked around by developing affinities, e.g. a consumer can keep reading from the same producer if it always has values. But if your load is so well-balanced that consumers could just pair with producers, you may as well do that instead.</ul><p>Additionally, orchestrating the addition/removal of individual queues in the channel and supporting sleeping becomes difficult.<h3 id=why-are-tunnel-channels-bad>Why are tunnel channels bad? <a href=#why-are-tunnel-channels-bad class=header-link>#</a></h3><p>Tunnels are the simplest MPMC channel: they hold no values and thus require a pairing between producer and consumer to transfer a value onto the consumer’s stack. Consequently, either the producer or consumer must sleep to accept the next value, every time. This is painfully slow.<h3 id=why-not-store-machine-word-sized-elements-in-the-channel>Why not store machine-word sized elements in the channel? <a href=#why-not-store-machine-word-sized-elements-in-the-channel class=header-link>#</a></h3><p>Instead of supporting arbitrarily sized values in the channel, what if we only accepted values that could fit in an atomic? More specifically pointers? Surprisingly, this doesn’t really help. If your only state is the array of atomic pointers, there’s no easy way to find free/filled slots. Thus, you need to go back to a circular buffer which has the same contention problems when the head/tail are updated but the slot hasn’t been atomically swapped to its new value. An alternative could be to scan the array for empty/filled slots until one is found, but under contention you’ll be fighting over the same slots.</div></div></div></div></div></div><footer class=text-defaults><small>Copyright © 2018-2025 Alex Saveau</small> <small><a href=//github.com/SUPERCILEX/personal-website>Source</a> | <a href=//twitter.com/SUPERCILEX>Twitter</a> | <a href=#top>Back to top</a></small></footer>